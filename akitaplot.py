import yaml
import os, sys, platform
import pandas as pd
import numpy as np
import colorama

def check_path_exists(path, errmsg):
  if not os.path.exists(path):
    print(str(path) + " is not existed! " + errmsg)
    sys.exit(0)
  return True

def get_expanded_abspath(path):
  # 1. expand ~ in path to absolute home directory
  # 2. normalize the path format, e.g. /path/to/dir// -> /path/to/dir
  return os.path.abspath(os.path.expanduser(path))

def print_green_prompt(msg):
  print(colorama.Fore.GREEN + colorama.Style.BRIGHT + msg + 
        colorama.Style.RESET_ALL)

# Load config YAML and organize models and benchmarks lists
class config:
  def __init__(self, config_path = './config.yaml'):
    # Declaring the variables as instance members, not class members
    self.models = []
    self.benchmarks = []
    self.grabs = []
    self.__raw_benchmarks = []
    self.__raw_config = []
    self.modelID = {}

    check_path_exists(config_path, "Please provide the configuration file!")

    with open(config_path, 'r', encoding='utf-8') as stream:
      try:
        yaml_str = yaml.safe_load(stream)
      except yaml.YAMLError as exc:
        print(exc)

    self.__raw_config = yaml_str['config']
    self.__raw_benchmarks = yaml_str['benchmarks']

    self.__generate_models_config()
    self.__generate_benchmarks_config()
    self.__generate_grabs_config()

    # print(self.models)

  def __generate_models_config(self):
    model_idx = 0

    # Baseline item
    prefix = get_expanded_abspath(self.__raw_config['baseline']['pathPrefix'])
    check_path_exists(prefix, "Please provide a valid path prefix of baseline!")
    baseline_display_name = self.__raw_config['baseline']['displayName']
    self.models.append({
      'name': 'baseline', 
      'displayName': baseline_display_name,
      'pathPrefix': prefix + '/' # build the full path prefix header
    })
    self.modelID['baseline'] = model_idx
    self.modelID[baseline_display_name] = model_idx
    model_idx += 1

    # Optimizations items
    for name, conf in self.__raw_config['optimizations'].items():
      prefix = get_expanded_abspath(conf['pathPrefix'])
      check_path_exists(prefix, "Please provide a valid path prefix of " +
        name + "!")
      self.models.append({
        'name': name,
        'displayName': conf['displayName'],
        'pathPrefix': prefix + '/' # build the full path prefix header
      })
      self.modelID[name] = self.modelID[conf['displayName']] = model_idx
      model_idx += 1

  def __generate_benchmarks_config(self):
    # TODO: add more configurable options for benchmarks
    for name, conf in self.__raw_benchmarks.items():
      self.benchmarks.append({
        'name': name,
        'displayName': conf['displayName'],
        'path': conf['path'],
        'options': conf['options']
      })
    # print(self.benchmarks)
  
  def __generate_grabs_config(self):
    for name, conf in self.__raw_config['grabs'].items():
      self.grabs.append({
        'name': name,
        'where': conf['where'],
        'what': conf['what']
      })
    # print(self.grabs)

  def get_baseline_config(self):
    return self.models[0]

  def yield_benchmark_of_model(self, model_name):
    for b in self.benchmarks:
      b_name = b['name']
      prefix = self.models[self.modelID[model_name]]['pathPrefix']
      suffix = self.__raw_benchmarks[b_name]['path']
      combine = prefix + suffix
      check_path_exists(combine, "Please check the combination of model " + 
        model_name + " prefix path and benchmark " + b_name + " prefix path!")
      # yield (benchmark_name, path, options)
      yield (b_name, combine, b['options'])

  def get_benchmark_options(self, benchmark_name):
    return self.__raw_benchmarks[benchmark_name]


# Build and run benchmark with certain options and generate metrics csv
class runner:
  def __init__(self, path, options):
    check_path_exists(path, "Please provide the right benchmark path!")
    cwd = os.getcwd()
    os.chdir(path)

    exe_name = os.path.basename(path)
    os_cmd = self.runner_command(exe_name, options)

    try:
      if os.system(os_cmd) != 0:
          raise Exception("Something wrong exists in the runner command " + 
            os_cmd + '!')
    except:
      print("Command does not work!")

    check_path_exists('metrics.csv', "metrics.csv not generated by benchmark.")
    self.metrics_csv_abspath = os.path.abspath('metrics.csv')
    os.chdir(cwd)
  
  def metrics_csv(self):
    return self.metrics_csv_abspath
  
  # Override it if needed
  def runner_command(self, exe_name, options):
    os_name = platform.system()
    if os_name == 'Linux' or os_name == 'Darwin':
      return "go build && ./" + exe_name + ' ' + options
    elif os_name == 'Windows':
      return "go build && " + exe_name + '.exe ' + options


# Append benchmark and build each model column for table
class model:
  def __init__(self, name:str, where:str, what:str):
    self.name, self.where, self.what = name, where, what
    self.df = pd.DataFrame()
    self.df[name] = ''
    # self.df.rename(index='Benchmarks', columns=name)
  
  def __repr__(self) -> str: 
    return self.df

  def __query(self, csv_path):
    # attention to the initial whitespaces of csv entities
    csv = pd.read_csv(csv_path, index_col=0, skipinitialspace=True, sep=r',')
    select = csv[(csv['where'] == self.where) & (csv['what'] == self.what)]
    # print(selected['value'].dtypes) => float64
    return np.float64(select['value'])

  def append_benchmark(self, benchmark_name, csv_path):
    self.df.loc[benchmark_name] = self.__query(csv_path)


class table:
  def __init__(self, title, models):
    self.title = title
    self.df = pd.concat([m.df for m in models], axis=1, join='outer')
    self.manipulation()

  def __repr__(self):
    return self.df

  def manipulation(self):
    pass

class plot:
  pass

class workflow:
  def __init__(self, config_path):
    self.conf = config(config_path)
    self.grab_tables = [self.generate_each_grab(g) for g in self.conf.grabs]
  
  def generate_each_grab(self, grab):
    models4table = []
    for each_model in self.conf.models:
      m = model(each_model['displayName'], grab['where'], grab['what'])
      for b_name, path, options in self.conf.yield_benchmark_of_model(m.name):
        print_green_prompt("Build [{}: {}] ...".format(m.name, b_name))
        csv_path = runner(path, options).metrics_csv()
        m.append_benchmark(b_name, csv_path)

      models4table.append(m)
      print(m.df)
    return self.transform_table(grab['name'], models4table)

  # Override it if needed, based on the derivative class of table
  def transform_table(self, table_name, models_for_table):
    return table(table_name, models_for_table)


class speedup_table(table):
  def manipulation(self):
    pass

class speedup(workflow):
  def transform_table(self, table_name, models_for_table):
    return speedup_table(table_name, models_for_table)

if __name__ == '__main__':
  # conf = config()
  # print(conf.get_benchmark_path_of_model("baseline", "fir"))
  speedup('./config.yaml')
